{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3b34b-c37f-4ba6-bc4b-f34c27992a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/\n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22cf944c-47ef-41aa-9732-467a9fbc4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85771e59-bcec-413f-b42d-ffaf034abe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbee6b5-3807-46b8-9c60-f56eac024434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4140d9c1-c0f2-4e50-af8a-7c121b836912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-G8D6EKQ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fef2d0c1c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a20052-2e39-4bfc-aee8-ada0c9a39c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read file in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9128ce98-9379-431a-b372-1beb07d4c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashish</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salaj</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phool</td>\n",
       "      <td>57.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shontu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name   Age  Experience\n",
       "0  Ashish  34.0        10.0\n",
       "1   Salaj  33.0         9.0\n",
       "2   Phool  57.0        37.0\n",
       "3  Shontu   1.0         NaN\n",
       "4     NaN  40.0         NaN"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excel = pd.read_excel('Write_xlsx.xlsx')\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be3ec7-ce95-4e40-a128-602db6eadcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56426069/how-to-read-xlsx-or-xls-files-as-spark-dataframe\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "# schema = StructType([StructField(\"Name\", StringType(), True), StructField(\"Age\", DoubleType(), True), StructField(\"Experience\", DoubleType(), True)])\n",
    "\n",
    "# df_excel_spark = spark.createDataFrame(df_excel, schema=schema)\n",
    "# df_excel_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c0124-b018-4976-9605-58347d438e8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read file in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9157ea6d-3e7a-400b-9162-bd441b9b07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|          40|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03ea44-2454-435b-9083-7d22b3532f9e",
   "metadata": {},
   "source": [
    "### Writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ada62-3a56-44a6-8846-9a9cc2161b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is not too heavy\n",
    "df.toPandas().to_csv('Write.csv', index=None)\n",
    "\n",
    "# It will create a folder, and inside that will create the file\n",
    "df.write.csv('Ashish', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a68bb4-8813-4af3-ad8d-fae223393db1",
   "metadata": {},
   "source": [
    "### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ba7f1642-9d9b-4df3-8c68-dfdf8c448959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+-----------------+------------------+\n",
      "|summary|  Name|Departments|           Salary|       Expenditure|\n",
      "+-------+------+-----------+-----------------+------------------+\n",
      "|  count|    12|         12|               11|                10|\n",
      "|   mean|  null|       40.0|6639.090909090909|            1825.0|\n",
      "| stddev|  null|       null|5569.094099663705|1349.1252309881722|\n",
      "|    min|Ashish|         40|               30|               500|\n",
      "|    max|Shontu|        IOT|            20000|              5000|\n",
      "+-------+------+-----------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33941a7-dd5d-47c7-9e74-f626551a22ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "31789c05-5765-4eb9-9292-fac62d3c8c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "be2f8265-24be-407f-b9fb-3b3692456657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Expenditure', 'int')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "66fc6c5d-80d3-470c-b779-5c2198365d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Name']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1d3e73ec-e574-4b01-aed1-8ea13987a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Expenditure: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f15eb-94b8-4544-99ff-a539e5e8e1e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to check Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "936329f5-4bd1-417e-ac0a-52bf10079676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dff018-39cb-4700-8aa4-da4f865da35a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to check no. of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e9b95862-8ec8-4164-be1d-61df20ef9e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Departments', 'Salary', 'Expenditure']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0882b41-3c48-41e8-81a3-4bd1318c7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   Name| Departments|\n",
      "+-------+------------+\n",
      "| Ashish|Data Science|\n",
      "| Ashish|         IOT|\n",
      "|  Salaj|    Big Data|\n",
      "| Ashish|    Big Data|\n",
      "|  Salaj|Data Science|\n",
      "|  Phool|Data Science|\n",
      "|  Phool|         IOT|\n",
      "|  Phool|    Big Data|\n",
      "|Sangwan|Data Science|\n",
      "|Sangwan|    Big Data|\n",
      "| Shontu|         IOT|\n",
      "|   null|          40|\n",
      "|Sangwan|        null|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['Name', 'Departments']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "25765ce1-ccb7-4b61-8ce1-4d669f6ac612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "| Ashish|\n",
      "| Ashish|\n",
      "|  Salaj|\n",
      "| Ashish|\n",
      "|  Salaj|\n",
      "|  Phool|\n",
      "|  Phool|\n",
      "|  Phool|\n",
      "|Sangwan|\n",
      "|Sangwan|\n",
      "| Shontu|\n",
      "|   null|\n",
      "|Sangwan|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4dcd3b57-5aae-4120-ba53-2942b7a413f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   Name| Departments|\n",
      "+-------+------------+\n",
      "| Ashish|Data Science|\n",
      "| Ashish|         IOT|\n",
      "|  Salaj|    Big Data|\n",
      "| Ashish|    Big Data|\n",
      "|  Salaj|Data Science|\n",
      "|  Phool|Data Science|\n",
      "|  Phool|         IOT|\n",
      "|  Phool|    Big Data|\n",
      "|Sangwan|Data Science|\n",
      "|Sangwan|    Big Data|\n",
      "| Shontu|         IOT|\n",
      "|   null|          40|\n",
      "|Sangwan|        null|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Name','Departments']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568bdfe-cfe7-44b1-b8ab-3d16dc5a3c8c",
   "metadata": {},
   "source": [
    "### How to change data type\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-change-column-type-in-pyspark-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fc828a64-ede0-44a2-8d75-2f79dcedffd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Expenditure', 'int')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f4b6f35c-9c16-4d36-9dc9-e9c2f1122c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'float'),\n",
       " ('Expenditure', 'int')]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One column at a time\n",
    "\n",
    "df1 = df.withColumn('Salary', df['Salary'].cast('float'))\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f5daa5b0-171a-4a69-b65d-b2789dab5fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'float'),\n",
       " ('Expenditure', 'float')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple columns - Using select we have to pass all columns whether we are casting it or not\n",
    "df1 = df.select(df['Name'], \n",
    "                df['Departments'], \n",
    "                df['Salary'].cast('float'), \n",
    "                df['Expenditure'].cast('float'))\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5979d32-b17a-487a-bd9b-ffaadc52f4a5",
   "metadata": {},
   "source": [
    "### How to check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5de11f63-8e82-4d3f-a6bf-88b9075f2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|          40|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d6590b44-d144-4189-94ec-39d98e8db46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+-----------+\n",
      "|Name|Departments|Salary|Expenditure|\n",
      "+----+-----------+------+-----------+\n",
      "|   0|          0|     0|          0|\n",
      "+----+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "068d1a58-7ca4-46f9-928f-f9c57fadd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+-----------+\n",
      "|Name|Departments|Salary|Expenditure|\n",
      "+----+-----------+------+-----------+\n",
      "|   1|          1|     2|          3|\n",
      "+----+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "53c5d8c4-8081-467a-92f1-8a08e61b97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+-----------+\n",
      "|Name|Departments|Salary|Expenditure|\n",
      "+----+-----------+------+-----------+\n",
      "|   1|          1|     2|          3|\n",
      "+----+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combining above 2\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c) | col(c).isNull() , c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7dcc94-6583-4782-9f64-b6887b9311cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check Unique Values in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ae671b19-992b-4eff-ab1e-0c84cbece767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "| Ashish|  34|        10|\n",
      "|  Salaj|  33|         9|\n",
      "|  Phool|  57|        37|\n",
      "| Shontu|   1|      null|\n",
      "|   null|  40|      null|\n",
      "|Sangwan|null|        30|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2c34aa78-0ac3-4920-9756-d89cbc047b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|   5|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "col_name = 'Name'\n",
    "df.select(countDistinct(df[col_name]).alias(col_name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "337c6aab-c576-4815-9cb5-91d338eed254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "+----+---+----------+\n",
      "|   5|  5|         4|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([countDistinct(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e545d6b-11d8-4965-a0bd-e1570ea56b63",
   "metadata": {},
   "source": [
    "### Value_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "08983f16-c068-40b1-b4d1-11f8c4fded9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|          40|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9a107433-580b-4142-8e36-58946303d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|count|\n",
      "+-------+-----+\n",
      "|Sangwan|    3|\n",
      "|  Phool|    3|\n",
      "| Ashish|    3|\n",
      "|  Salaj|    2|\n",
      "|   null|    1|\n",
      "| Shontu|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09459c-00d8-4a53-bdd8-40a065b4b5ed",
   "metadata": {},
   "source": [
    "### Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fac0d4e8-9425-4e1c-ab7a-fbfc7231946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|          40|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fb8e90e1-9fe7-4d39-87df-f2538fc0841e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+\n",
      "|         Name| Age|Experience|\n",
      "+-------------+----+----------+\n",
      "|       Ashish|  34|        10|\n",
      "|        Salaj|  33|         9|\n",
      "|        Phool|  57|        37|\n",
      "|       Shontu|   1|      null|\n",
      "|Missing Value|  40|      null|\n",
      "|      Sangwan|null|        30|\n",
      "+-------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will replace only null in string data type column\n",
    "df.na.fill('Missing Value').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "39e977c5-9d9e-441b-ac43-8ec88a2da450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Experience|\n",
      "+-------+---+----------+\n",
      "| Ashish| 34|        10|\n",
      "|  Salaj| 33|         9|\n",
      "|  Phool| 57|        37|\n",
      "| Shontu|  1|        50|\n",
      "|   null| 40|        50|\n",
      "|Sangwan| 50|        30|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will replace only null in int data type column\n",
    "df.na.fill(50).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ab2c674c-b7f2-48ae-b0e3-54d285ed6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Experience|\n",
      "+-------+---+----------+\n",
      "| Ashish| 34|        10|\n",
      "|  Salaj| 33|         9|\n",
      "|  Phool| 57|        37|\n",
      "| Shontu|  1|      null|\n",
      "|   null| 40|      null|\n",
      "|Sangwan| 40|        30|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(40, subset=['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "675e1694-d6f5-4289-b1a1-7bb1644de169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+\n",
      "|         Name|Age|Experience|\n",
      "+-------------+---+----------+\n",
      "|       Ashish| 34|        10|\n",
      "|        Salaj| 33|         9|\n",
      "|        Phool| 57|        37|\n",
      "|       Shontu|  1|        20|\n",
      "|Missing Value| 40|        20|\n",
      "|      Sangwan| 50|        30|\n",
      "+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({'Name':'Missing Value', 'Age':50, 'Experience':20}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10fab82d-54e4-4255-ac83-0eddcc32aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+------------------+\n",
      "|   Name| Age|Experience|Age_imputed|Experience_imputed|\n",
      "+-------+----+----------+-----------+------------------+\n",
      "| Ashish|  34|        10|         34|                10|\n",
      "|  Salaj|  33|         9|         33|                 9|\n",
      "|  Phool|  57|        37|         57|                37|\n",
      "| Shontu|   1|      null|          1|                21|\n",
      "|   null|  40|      null|         40|                21|\n",
      "|Sangwan|null|        30|         33|                30|\n",
      "+-------+----+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['Age','Experience'], \n",
    "                  outputCols=['{}_imputed'.format(cols) for cols in ['Age','Experience']]).setStrategy('mean')\n",
    "\n",
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bb886-6da4-4c8a-863d-115cb418f146",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f845ed9b-2667-45f4-abe5-b3547628ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "| Ashish|  34|        10|\n",
      "|  Salaj|  33|         9|\n",
      "|  Phool|  57|        37|\n",
      "| Shontu|   1|      null|\n",
      "|   null|  40|      null|\n",
      "|Sangwan|null|        30|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5812a8e3-8f94-42b8-9457-7459d441e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|Ashish| 34|        10|\n",
      "| Salaj| 33|         9|\n",
      "| Phool| 57|        37|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Be default, how='any'\n",
    "df.na.drop().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3ebd536-dc8e-4401-a988-182d7fb88922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "| Ashish|  34|        10|\n",
      "|  Salaj|  33|         9|\n",
      "|  Phool|  57|        37|\n",
      "| Shontu|   1|      null|\n",
      "|   null|  40|      null|\n",
      "|Sangwan|null|        30|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1db9f99-162d-4705-9e56-6202c170bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "| Ashish|  34|        10|\n",
      "|  Salaj|  33|         9|\n",
      "|  Phool|  57|        37|\n",
      "| Shontu|   1|      null|\n",
      "|Sangwan|null|        30|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It will not drop the row if it has atleast 2 non-null\n",
    "df.na.drop(thresh=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8496586e-b9cc-4bd7-a285-33aa6feaaf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   Name| Age|Experience|\n",
      "+-------+----+----------+\n",
      "| Ashish|  34|        10|\n",
      "|  Salaj|  33|         9|\n",
      "|  Phool|  57|        37|\n",
      "| Shontu|   1|      null|\n",
      "|Sangwan|null|        30|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It will delete the row if mentioned columns has any null value\n",
    "df.na.drop(how='any', subset=['Name']).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab754c50-2cfd-4636-9adc-6fafb736a5a0",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b92b1c86-7690-4ff2-b50f-eb2128c4eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|          40|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f138671e-5620-4ca5-baec-e209ee7e5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|   null|          40|  null|       null|\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.dropDuplicates()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f5d27fbb-c791-46fe-b367-36b3de440202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "|   null|          40|  null|       null|\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.dropDuplicates(['Name'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9be497b6-5bf9-4304-ac05-6736a0612bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.dropDuplicates(['Salary', 'Expenditure'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894360e-10c2-4c2c-beb4-714858f31ee9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Groupby and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4d782125-a8f6-4318-a7d8-4b8887abfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "58cb1369-1de3-4b92-bf51-b2ae2332fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+----------------+\n",
      "|   Name| Departments|sum(Salary)|avg(Expenditure)|\n",
      "+-------+------------+-----------+----------------+\n",
      "|  Phool|Data Science|      20000|          5000.0|\n",
      "| Shontu|         IOT|       null|            null|\n",
      "|  Phool|         IOT|      10000|          2500.0|\n",
      "| Ashish|    Big Data|       4000|          1000.0|\n",
      "|   null|         IOT|       null|            null|\n",
      "|Sangwan|Data Science|      10000|          2500.0|\n",
      "|Sangwan|    Big Data|       2000|           500.0|\n",
      "|Sangwan|        null|         30|            null|\n",
      "|  Phool|    Big Data|       5000|          1250.0|\n",
      "| Ashish|         IOT|       5000|          1250.0|\n",
      "| Ashish|Data Science|      10000|          2500.0|\n",
      "|  Salaj|    Big Data|       4000|          1000.0|\n",
      "|  Salaj|Data Science|       3000|           750.0|\n",
      "+-------+------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('Name','Departments').agg({'Salary':'sum', 'Expenditure':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fbebe9a7-ddb7-4ddf-8ef5-fa958954b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------------+\n",
      "| Departments|sum(Salary)|avg(Expenditure)|\n",
      "+------------+-----------+----------------+\n",
      "|         IOT|      15000|          1875.0|\n",
      "|        null|         30|            null|\n",
      "|    Big Data|      15000|           937.5|\n",
      "|Data Science|      43000|          2687.5|\n",
      "+------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('Departments').agg({'Salary':'sum', 'Expenditure':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "99f9bd6e-f7f6-4185-a779-9ddab1e6caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+\n",
      "| Departments|avg(Salary)|count(Salary)|\n",
      "+------------+-----------+-------------+\n",
      "|         IOT|     7500.0|            2|\n",
      "|        null|       30.0|            1|\n",
      "|    Big Data|     3750.0|            4|\n",
      "|Data Science|    10750.0|            4|\n",
      "+------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df1.groupBy(\"Departments\").agg(F.mean('Salary'), F.count('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d5d47b8e-a892-4463-ab2e-07aa3b8a2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+----------------+\n",
      "| Departments|avg(Salary)|count(Salary)|avg(Expenditure)|\n",
      "+------------+-----------+-------------+----------------+\n",
      "|         IOT|     7500.0|            2|          1875.0|\n",
      "|        null|       30.0|            1|            null|\n",
      "|    Big Data|     3750.0|            4|           937.5|\n",
      "|Data Science|    10750.0|            4|          2687.5|\n",
      "+------------+-----------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df1.groupBy(\"Departments\").agg(F.mean('Salary'), F.count('Salary'),\n",
    "                               F.mean('Expenditure')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad1f1f-e240-43e4-a3af-5b76f04d1077",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Replace\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "85dc12d9-e46b-4862-a758-e811a62350c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "76841aee-40b9-416c-a6ce-0c99af6a0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+-----------+\n",
      "|   Name|       Departments|Salary|Expenditure|\n",
      "+-------+------------------+------+-----------+\n",
      "| Ashish|      Data Science| 10000|       2500|\n",
      "| Ashish|Internet of Things|  5000|       1250|\n",
      "|  Salaj|          Big Data|  4000|       1000|\n",
      "| Ashish|          Big Data|  4000|       1000|\n",
      "|  Salaj|      Data Science|  3000|        750|\n",
      "|  Phool|      Data Science| 20000|       5000|\n",
      "|  Phool|Internet of Things| 10000|       2500|\n",
      "|  Phool|          Big Data|  5000|       1250|\n",
      "|Sangwan|      Data Science| 10000|       2500|\n",
      "|Sangwan|          Big Data|  2000|        500|\n",
      "| Shontu|Internet of Things|  null|       null|\n",
      "|   null|Internet of Things|  null|       null|\n",
      "|Sangwan|              null|    30|       null|\n",
      "+-------+------------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn('Departments', regexp_replace('Departments', 'IOT', 'Internet of Things')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "db299abe-56bf-4a1c-b20e-8039ecf900ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+-----------+\n",
      "|   Name|Departments|Salary|Expenditure|\n",
      "+-------+-----------+------+-----------+\n",
      "| Ashish|         DS| 10000|       2500|\n",
      "| Ashish|        IOT|  5000|       1250|\n",
      "|  Salaj|         BG|  4000|       1000|\n",
      "| Ashish|         BG|  4000|       1000|\n",
      "|  Salaj|         DS|  3000|        750|\n",
      "|  Phool|         DS| 20000|       5000|\n",
      "|  Phool|        IOT| 10000|       2500|\n",
      "|  Phool|         BG|  5000|       1250|\n",
      "|Sangwan|         DS| 10000|       2500|\n",
      "|Sangwan|         BG|  2000|        500|\n",
      "| Shontu|        IOT|  null|       null|\n",
      "|   null|        IOT|  null|       null|\n",
      "|Sangwan|       null|    30|       null|\n",
      "+-------+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapping = {'Data Science': 'DS', 'Big Data': 'BG'}\n",
    "df.replace(mapping, subset=['Departments']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf009e63-68f6-4b6f-921d-104a9ac39bdb",
   "metadata": {},
   "source": [
    "### Filter Operation / Data Slicing\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-where-filter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a2665f4e-d94a-453c-ad4b-4b0cc785e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "16cee6ed-065d-4043-9ffd-b924cc266c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+-----------+\n",
      "|Name|Departments|Salary|Expenditure|\n",
      "+----+-----------+------+-----------+\n",
      "|null|        IOT|  null|       null|\n",
      "+----+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Name'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5270e9ee-3ec2-4c91-9fe9-3c877c2ba924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+-----------+\n",
      "|  Name| Departments|Salary|Expenditure|\n",
      "+------+------------+------+-----------+\n",
      "|Ashish|Data Science| 10000|       2500|\n",
      "|Ashish|         IOT|  5000|       1250|\n",
      "|Ashish|    Big Data|  4000|       1000|\n",
      "+------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Name']=='Ashish').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8598ed77-2b94-406d-be0f-97d6d7b7da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+-----------+\n",
      "|  Name| Departments|Salary|Expenditure|\n",
      "+------+------------+------+-----------+\n",
      "|Ashish|Data Science| 10000|       2500|\n",
      "+------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Name']=='Ashish') & (df['Departments']=='Data Science')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2dcb22fe-b54c-436b-a661-535bf47ca3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-----------+\n",
      "|  Name|Departments|Salary|Expenditure|\n",
      "+------+-----------+------+-----------+\n",
      "|Ashish|        IOT|  5000|       1250|\n",
      "|Ashish|   Big Data|  4000|       1000|\n",
      "+------+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Name']=='Ashish') & (df['Departments']!='Data Science')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "708a845b-accb-4663-a860-e18b869be71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-----------+\n",
      "|  Name|Departments|Salary|Expenditure|\n",
      "+------+-----------+------+-----------+\n",
      "|Ashish|        IOT|  5000|       1250|\n",
      "|Ashish|   Big Data|  4000|       1000|\n",
      "+------+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Name']=='Ashish') & ~(df['Departments']=='Data Science')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2f16e026-a6ca-450a-81c6-7996404d6811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-----------+\n",
      "|  Name|Departments|Salary|Expenditure|\n",
      "+------+-----------+------+-----------+\n",
      "|Ashish|        IOT|  5000|       1250|\n",
      "|Ashish|   Big Data|  4000|       1000|\n",
      "+------+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Name']=='Ashish') & (df['Departments'].isin(['IOT','Big Data']))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "330a95f5-0be7-470b-ac18-9b24f0e4e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Salary']>=2000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd219e87-a898-483c-91aa-2acf43c168a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select Specific columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "fab686e8-d653-4941-b3a8-bb606e7f0f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a6db3923-a171-4551-8b32-36d944cc4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   Name| Departments|\n",
      "+-------+------------+\n",
      "| Ashish|Data Science|\n",
      "| Ashish|         IOT|\n",
      "|  Salaj|    Big Data|\n",
      "| Ashish|    Big Data|\n",
      "|  Salaj|Data Science|\n",
      "|  Phool|Data Science|\n",
      "|  Phool|         IOT|\n",
      "|  Phool|    Big Data|\n",
      "|Sangwan|Data Science|\n",
      "|Sangwan|    Big Data|\n",
      "| Shontu|         IOT|\n",
      "|   null|         IOT|\n",
      "|Sangwan|        null|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['Name','Departments']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2b09dc8c-cbeb-42e2-9d8b-7b2d9a4f3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   Name| Departments|\n",
      "+-------+------------+\n",
      "| Ashish|Data Science|\n",
      "| Ashish|         IOT|\n",
      "|  Salaj|    Big Data|\n",
      "| Ashish|    Big Data|\n",
      "|  Salaj|Data Science|\n",
      "|  Phool|Data Science|\n",
      "|  Phool|         IOT|\n",
      "|  Phool|    Big Data|\n",
      "|Sangwan|Data Science|\n",
      "|Sangwan|    Big Data|\n",
      "| Shontu|         IOT|\n",
      "|   null|         IOT|\n",
      "|Sangwan|        null|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name','Departments').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "35afc951-8990-4640-81a6-da3d359fe61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "| Departments|Salary|\n",
      "+------------+------+\n",
      "|Data Science| 10000|\n",
      "|         IOT|  5000|\n",
      "|    Big Data|  4000|\n",
      "|    Big Data|  4000|\n",
      "|Data Science|  3000|\n",
      "|Data Science| 20000|\n",
      "|         IOT| 10000|\n",
      "|    Big Data|  5000|\n",
      "|Data Science| 10000|\n",
      "|    Big Data|  2000|\n",
      "|         IOT|  null|\n",
      "|         IOT|  null|\n",
      "|        null|    30|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[1:3]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab9e8b-c5b5-43b9-bcff-f41c2d388b3a",
   "metadata": {},
   "source": [
    "### Add new column\n",
    "https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/#:~:text=Add%20New%20Column%20with%20Constant,None%20use%20lit(None)%20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ebaa984c-0cf0-4f5f-8d01-09228c553662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "bfca0f68-60c0-4242-830f-670a5e312810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+----------+\n",
      "|   Name| Departments|Salary|Expenditure|New Salary|\n",
      "+-------+------------+------+-----------+----------+\n",
      "| Ashish|Data Science| 10000|       2500|     10002|\n",
      "| Ashish|         IOT|  5000|       1250|      5002|\n",
      "|  Salaj|    Big Data|  4000|       1000|      4002|\n",
      "| Ashish|    Big Data|  4000|       1000|      4002|\n",
      "|  Salaj|Data Science|  3000|        750|      3002|\n",
      "|  Phool|Data Science| 20000|       5000|     20002|\n",
      "|  Phool|         IOT| 10000|       2500|     10002|\n",
      "|  Phool|    Big Data|  5000|       1250|      5002|\n",
      "|Sangwan|Data Science| 10000|       2500|     10002|\n",
      "|Sangwan|    Big Data|  2000|        500|      2002|\n",
      "| Shontu|         IOT|  null|       null|      null|\n",
      "|   null|         IOT|  null|       null|      null|\n",
      "|Sangwan|        null|    30|       null|        32|\n",
      "+-------+------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New Salary', df['Salary']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1c61195d-bfe6-431d-831b-d03cbcd1c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+----------+\n",
      "|   Name| Departments|Salary|Expenditure|New Salary|\n",
      "+-------+------------+------+-----------+----------+\n",
      "| Ashish|Data Science| 10000|       2500|      same|\n",
      "| Ashish|         IOT|  5000|       1250|      same|\n",
      "|  Salaj|    Big Data|  4000|       1000|      same|\n",
      "| Ashish|    Big Data|  4000|       1000|      same|\n",
      "|  Salaj|Data Science|  3000|        750|      same|\n",
      "|  Phool|Data Science| 20000|       5000|      same|\n",
      "|  Phool|         IOT| 10000|       2500|      same|\n",
      "|  Phool|    Big Data|  5000|       1250|      same|\n",
      "|Sangwan|Data Science| 10000|       2500|      same|\n",
      "|Sangwan|    Big Data|  2000|        500|      same|\n",
      "| Shontu|         IOT|  null|       null|      same|\n",
      "|   null|         IOT|  null|       null|      same|\n",
      "|Sangwan|        null|    30|       null|      same|\n",
      "+-------+------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn('New Salary', lit('same')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "61850209-68f2-4ecf-9b03-6561758f30d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+----------+\n",
      "|   Name| Departments|Salary|Expenditure|New Salary|\n",
      "+-------+------------+------+-----------+----------+\n",
      "| Ashish|Data Science| 10000|       2500|        10|\n",
      "| Ashish|         IOT|  5000|       1250|        10|\n",
      "|  Salaj|    Big Data|  4000|       1000|        10|\n",
      "| Ashish|    Big Data|  4000|       1000|        10|\n",
      "|  Salaj|Data Science|  3000|        750|        10|\n",
      "|  Phool|Data Science| 20000|       5000|        10|\n",
      "|  Phool|         IOT| 10000|       2500|        10|\n",
      "|  Phool|    Big Data|  5000|       1250|        10|\n",
      "|Sangwan|Data Science| 10000|       2500|        10|\n",
      "|Sangwan|    Big Data|  2000|        500|        10|\n",
      "| Shontu|         IOT|  null|       null|        10|\n",
      "|   null|         IOT|  null|       null|        10|\n",
      "|Sangwan|        null|    30|       null|        10|\n",
      "+-------+------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New Salary', lit(10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fe46ace3-c245-499a-b94f-5cf99db2d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+---------------+\n",
      "|   Name| Departments|Salary|Expenditure|New Departments|\n",
      "+-------+------------+------+-----------+---------------+\n",
      "| Ashish|Data Science| 10000|       2500|             DS|\n",
      "| Ashish|         IOT|  5000|       1250|            IOT|\n",
      "|  Salaj|    Big Data|  4000|       1000|       Big Data|\n",
      "| Ashish|    Big Data|  4000|       1000|       Big Data|\n",
      "|  Salaj|Data Science|  3000|        750|   Data Science|\n",
      "|  Phool|Data Science| 20000|       5000|   Data Science|\n",
      "|  Phool|         IOT| 10000|       2500|            IOT|\n",
      "|  Phool|    Big Data|  5000|       1250|       Big Data|\n",
      "|Sangwan|Data Science| 10000|       2500|   Data Science|\n",
      "|Sangwan|    Big Data|  2000|        500|       Big Data|\n",
      "| Shontu|         IOT|  null|       null|            IOT|\n",
      "|   null|         IOT|  null|       null|            IOT|\n",
      "|Sangwan|        null|    30|       null|           null|\n",
      "+-------+------------+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New Departments', when((df['Name']=='Ashish') & (df['Departments']=='Data Science'),'DS').\n",
    "                                 otherwise(df['Departments'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4aeb2405-62c7-4a68-bfb5-072f299afdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+---------------+\n",
      "|   Name| Departments|Salary|Expenditure|New Departments|\n",
      "+-------+------------+------+-----------+---------------+\n",
      "| Ashish|Data Science| 10000|       2500|             DS|\n",
      "| Ashish|         IOT|  5000|       1250|            IOT|\n",
      "|  Salaj|    Big Data|  4000|       1000|             BG|\n",
      "| Ashish|    Big Data|  4000|       1000|             BG|\n",
      "|  Salaj|Data Science|  3000|        750|             DS|\n",
      "|  Phool|Data Science| 20000|       5000|             DS|\n",
      "|  Phool|         IOT| 10000|       2500|            IOT|\n",
      "|  Phool|    Big Data|  5000|       1250|             BG|\n",
      "|Sangwan|Data Science| 10000|       2500|             DS|\n",
      "|Sangwan|    Big Data|  2000|        500|             BG|\n",
      "| Shontu|         IOT|  null|       null|            IOT|\n",
      "|   null|         IOT|  null|       null|            IOT|\n",
      "|Sangwan|        null|    30|       null|             --|\n",
      "+-------+------------+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New Departments', when(df['Departments']=='Data Science','DS').\n",
    "                                 when(df['Departments']=='Big Data','BG').\n",
    "                                 when(df['Departments'].isNull(),'--').\n",
    "                                 otherwise(df['Departments'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f0e4a-90c4-45de-9d69-217fdaad3a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66199df9-d1fb-4310-a900-5676fab3b394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3e4bf-137f-468c-bbe6-327fa42556c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6caed3-01c2-4bfc-b696-3054b8a1bb4e",
   "metadata": {},
   "source": [
    "### Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7c533862-88ae-427b-9796-b605e101e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f1e87ba1-8f6d-4037-b10a-590468bb6dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   Name| Departments|Salary|\n",
      "+-------+------------+------+\n",
      "| Ashish|Data Science| 10000|\n",
      "| Ashish|         IOT|  5000|\n",
      "|  Salaj|    Big Data|  4000|\n",
      "| Ashish|    Big Data|  4000|\n",
      "|  Salaj|Data Science|  3000|\n",
      "|  Phool|Data Science| 20000|\n",
      "|  Phool|         IOT| 10000|\n",
      "|  Phool|    Big Data|  5000|\n",
      "|Sangwan|Data Science| 10000|\n",
      "|Sangwan|    Big Data|  2000|\n",
      "| Shontu|         IOT|  null|\n",
      "|   null|         IOT|  null|\n",
      "|Sangwan|        null|    30|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Expenditure').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dd62b4ee-bef9-40dd-bb59-87c8b97e8f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   Name| Departments|\n",
      "+-------+------------+\n",
      "| Ashish|Data Science|\n",
      "| Ashish|         IOT|\n",
      "|  Salaj|    Big Data|\n",
      "| Ashish|    Big Data|\n",
      "|  Salaj|Data Science|\n",
      "|  Phool|Data Science|\n",
      "|  Phool|         IOT|\n",
      "|  Phool|    Big Data|\n",
      "|Sangwan|Data Science|\n",
      "|Sangwan|    Big Data|\n",
      "| Shontu|         IOT|\n",
      "|   null|         IOT|\n",
      "|Sangwan|        null|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Salary','Expenditure').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5010c9-2ba7-4743-b587-9ec0ac8c2509",
   "metadata": {},
   "source": [
    "### Add Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5eff74f3-6bb7-4cd2-bbce-a1b451e01248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+-----------+\n",
      "|   Name| Departments|Salary|Expenditure|\n",
      "+-------+------------+------+-----------+\n",
      "| Ashish|Data Science| 10000|       2500|\n",
      "| Ashish|         IOT|  5000|       1250|\n",
      "|  Salaj|    Big Data|  4000|       1000|\n",
      "| Ashish|    Big Data|  4000|       1000|\n",
      "|  Salaj|Data Science|  3000|        750|\n",
      "|  Phool|Data Science| 20000|       5000|\n",
      "|  Phool|         IOT| 10000|       2500|\n",
      "|  Phool|    Big Data|  5000|       1250|\n",
      "|Sangwan|Data Science| 10000|       2500|\n",
      "|Sangwan|    Big Data|  2000|        500|\n",
      "| Shontu|         IOT|  null|       null|\n",
      "|   null|         IOT|  null|       null|\n",
      "|Sangwan|        null|    30|       null|\n",
      "+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4d3e992-e790-4fbc-a852-b6095c8784f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+-------------------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|                New|\n",
      "+----------+-------+------------+------+-----------+-------------------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500| AshishData Science|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|          AshishIOT|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|      SalajBig Data|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|     AshishBig Data|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|  SalajData Science|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|  PhoolData Science|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|           PhoolIOT|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|      PhoolBig Data|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|SangwanData Science|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|    SangwanBig Data|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|          ShontuIOT|\n",
      "|2022-02-22|   null|         IOT|  null|       null|               null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|               null|\n",
      "+----------+-------+------------+------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, concat_ws\n",
    "df.withColumn('New', concat(df['Name'], df['Departments'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "044491f9-9a90-4729-97d1-8bc06fcea5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+--------------------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|                 New|\n",
      "+----------+-------+------------+------+-----------+--------------------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500| Ashish_Data Science|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|          Ashish_IOT|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|      Salaj_Big Data|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|     Ashish_Big Data|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|  Salaj_Data Science|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|  Phool_Data Science|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|           Phool_IOT|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|      Phool_Big Data|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|Sangwan_Data Science|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|    Sangwan_Big Data|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|          Shontu_IOT|\n",
      "|2022-02-22|   null|         IOT|  null|       null|                 IOT|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|             Sangwan|\n",
      "+----------+-------+------------+------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New', concat_ws('_',df['Name'], df['Departments'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb39054-9dab-44d1-930e-ff0d2e841570",
   "metadata": {},
   "source": [
    "### Rename Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "01e4deba-9539-451a-8022-e452bdb315d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+-----------+\n",
      "|New Name| Departments|Salary|Expenditure|\n",
      "+--------+------------+------+-----------+\n",
      "|  Ashish|Data Science| 10000|       2500|\n",
      "|  Ashish|         IOT|  5000|       1250|\n",
      "|   Salaj|    Big Data|  4000|       1000|\n",
      "|  Ashish|    Big Data|  4000|       1000|\n",
      "|   Salaj|Data Science|  3000|        750|\n",
      "|   Phool|Data Science| 20000|       5000|\n",
      "|   Phool|         IOT| 10000|       2500|\n",
      "|   Phool|    Big Data|  5000|       1250|\n",
      "| Sangwan|Data Science| 10000|       2500|\n",
      "| Sangwan|    Big Data|  2000|        500|\n",
      "|  Shontu|         IOT|  null|       null|\n",
      "|    null|         IOT|  null|       null|\n",
      "| Sangwan|        null|    30|       null|\n",
      "+--------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149bef1-9fa8-4d95-849c-f608ffe7033d",
   "metadata": {},
   "source": [
    "### Date Format\n",
    "\n",
    "https://www.datasciencemadesimple.com/get-month-year-and-quarter-from-date-in-pyspark/\n",
    "https://www.datasciencemadesimple.com/subtract-or-add-days-months-and-years-to-timestamp-in-pyspark/\n",
    "\n",
    "By default, Date has to be in this 2022-02-11 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c569c4-e844-45bf-aad6-257064c470d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|\n",
      "+----------+-------+------------+------+-----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|\n",
      "|2022-02-22|   null|         IOT|  null|       null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|\n",
      "+----------+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a20506-e8a0-479b-a91c-209c221de4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Expenditure', 'int')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab4df99-8e78-46ec-8cfd-2179580c5832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Expenditure', 'int'),\n",
       " ('New_Date', 'date')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, to_utc_timestamp, dayofmonth\n",
    "\n",
    "df1 = df.withColumn('New_Date', to_date('Date'))\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac5da91-15aa-45ec-bbc6-af67be96c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|  New_Date|\n",
      "+----------+-------+------------+------+-----------+----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|2022-02-11|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|2022-02-12|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|2022-02-13|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|2022-02-14|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|2022-02-15|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|2022-02-16|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|2022-02-17|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|2022-02-18|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|2022-02-19|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|2022-02-20|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|2022-02-21|\n",
      "|2022-02-22|   null|         IOT|  null|       null|2022-02-22|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|2022-02-23|\n",
      "+----------+-------+------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0742bc62-3356-4f5e-8556-414791b18fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, to_utc_timestamp, datediff\n",
    "from pyspark.sql.functions import dayofweek, dayofmonth, dayofyear, date_format, year, month, quarter , weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e565298a-17f2-49f2-ac8b-21a4f10bbaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+-------+---------+---------+---------+\n",
      "|  New_Date|Year|Month|Day|Week|Quarter|DayOfWeek|DayOfYear|  Weekday|\n",
      "+----------+----+-----+---+----+-------+---------+---------+---------+\n",
      "|2022-02-11|2022|    2| 11|   6|      1|        6|       42|   Friday|\n",
      "|2022-02-12|2022|    2| 12|   6|      1|        7|       43| Saturday|\n",
      "|2022-02-13|2022|    2| 13|   6|      1|        1|       44|   Sunday|\n",
      "|2022-02-14|2022|    2| 14|   7|      1|        2|       45|   Monday|\n",
      "|2022-02-15|2022|    2| 15|   7|      1|        3|       46|  Tuesday|\n",
      "|2022-02-16|2022|    2| 16|   7|      1|        4|       47|Wednesday|\n",
      "|2022-02-17|2022|    2| 17|   7|      1|        5|       48| Thursday|\n",
      "|2022-02-18|2022|    2| 18|   7|      1|        6|       49|   Friday|\n",
      "|2022-02-19|2022|    2| 19|   7|      1|        7|       50| Saturday|\n",
      "|2022-02-20|2022|    2| 20|   7|      1|        1|       51|   Sunday|\n",
      "|2022-02-21|2022|    2| 21|   8|      1|        2|       52|   Monday|\n",
      "|2022-02-22|2022|    2| 22|   8|      1|        3|       53|  Tuesday|\n",
      "|2022-02-23|2022|    2| 23|   8|      1|        4|       54|Wednesday|\n",
      "+----------+----+-----+---+----+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('Year', year(df1['New_Date'])) \\\n",
    "           .withColumn('Month', month(df1['New_Date'])) \\\n",
    "           .withColumn('Day', dayofmonth(df1['New_Date'])) \\\n",
    "           .withColumn('Week', weekofyear(df1['New_Date'])) \\\n",
    "           .withColumn('Quarter', quarter(df1['New_Date'])) \\\n",
    "           .withColumn('DayOfWeek', dayofweek(df1['New_Date'])) \\\n",
    "           .withColumn('DayOfYear', dayofyear(df1['New_Date'])) \\\n",
    "           .withColumn('Weekday', date_format(df1['New_Date'], \"EEEE\"))\n",
    "\n",
    "df2.select(df2.columns[5:]).show()\n",
    "\n",
    "# 1st day os week is Sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5eaf1-0e14-462b-b0e5-7dfb0cd59c92",
   "metadata": {},
   "source": [
    "### Difference between 2 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa000f42-bb78-40ec-b931-428f32fd2351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|\n",
      "+----------+-------+------------+------+-----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|\n",
      "|2022-02-22|   null|         IOT|  null|       null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|\n",
      "+----------+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bef9c2e-2083-4ce7-ac8f-e604eff2962b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Expenditure', 'int')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn('Date', to_date(df['Date']))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5e6a4f9-5c57-4fb3-814e-b48d8c6dcd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|  New_Date|\n",
      "+----------+-------+------------+------+-----------+----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|2022-02-12|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|2022-02-13|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|2022-02-14|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|2022-02-15|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|2022-02-16|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|2022-02-17|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|2022-02-18|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|2022-02-19|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|2022-02-20|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|2022-02-21|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|2022-02-22|\n",
      "|2022-02-22|   null|         IOT|  null|       null|2022-02-23|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|2022-02-24|\n",
      "+----------+-------+------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('New_Date', df['Date']+1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa422662-f1a5-4436-83a8-17c8f1428d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+----------+--------------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|  New_Date|DateDifference|\n",
      "+----------+-------+------------+------+-----------+----------+--------------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|2022-02-12|             1|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|2022-02-13|             1|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|2022-02-14|             1|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|2022-02-15|             1|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|2022-02-16|             1|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|2022-02-17|             1|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|2022-02-18|             1|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|2022-02-19|             1|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|2022-02-20|             1|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|2022-02-21|             1|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|2022-02-22|             1|\n",
      "|2022-02-22|   null|         IOT|  null|       null|2022-02-23|             1|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|2022-02-24|             1|\n",
      "+----------+-------+------------+------+-----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('DateDifference', datediff(df['New_Date'], df['Date']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef477ad0-d4ec-4211-ab0c-ecef9acc07ae",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6428fafc-7a5d-4727-abeb-cea823c5e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|\n",
      "+----------+-------+------------+------+-----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|\n",
      "|2022-02-22|   null|         IOT|  null|       null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|\n",
      "+----------+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "979af997-f98d-43b1-b2a7-4d6038d8ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|\n",
      "+----------+-------+------------+------+-----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|\n",
      "|2022-02-22|   null|         IOT|  null|       null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|\n",
      "+----------+-------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('Test_Data1.csv', header=True, inferSchema=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80d35596-4939-4ba4-8574-324be797b938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------+-----------+\n",
      "|      Date|   Name| Departments|Salary|Expenditure|\n",
      "+----------+-------+------------+------+-----------+\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "|2022-02-18|  Phool|    Big Data|  5000|       1250|\n",
      "|2022-02-19|Sangwan|Data Science| 10000|       2500|\n",
      "|2022-02-20|Sangwan|    Big Data|  2000|        500|\n",
      "|2022-02-21| Shontu|         IOT|  null|       null|\n",
      "|2022-02-22|   null|         IOT|  null|       null|\n",
      "|2022-02-23|Sangwan|        null|    30|       null|\n",
      "|2022-02-11| Ashish|Data Science| 10000|       2500|\n",
      "|2022-02-12| Ashish|         IOT|  5000|       1250|\n",
      "|2022-02-13|  Salaj|    Big Data|  4000|       1000|\n",
      "|2022-02-14| Ashish|    Big Data|  4000|       1000|\n",
      "|2022-02-15|  Salaj|Data Science|  3000|        750|\n",
      "|2022-02-16|  Phool|Data Science| 20000|       5000|\n",
      "|2022-02-17|  Phool|         IOT| 10000|       2500|\n",
      "+----------+-------+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.unionByName(df2)\n",
    "df3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
